{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13245139,"sourceType":"datasetVersion","datasetId":8392226}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade transformers datasets accelerate bitsandbytes peft trl wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:17:02.997437Z","iopub.execute_input":"2025-10-02T20:17:02.997718Z","iopub.status.idle":"2025-10-02T20:19:25.984736Z","shell.execute_reply.started":"2025-10-02T20:17:02.997687Z","shell.execute_reply":"2025-10-02T20:19:25.984053Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport wandb\n\n# Get your API key from environment variable\n# api_key = os.getenv('WANDB_API_KEY')\n\n# Log in to wandb using the API key\nkey_rg = '89cd96b19daeabb992898cba0e7a488d695cea6c'\nkey_thil = '2e7dcb25a5297b3d9df254f7acbb16b3b15fab6a'\nwandb.login(key=key_thil)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:19:25.985722Z","iopub.execute_input":"2025-10-02T20:19:25.985952Z","iopub.status.idle":"2025-10-02T20:19:33.973256Z","shell.execute_reply.started":"2025-10-02T20:19:25.985925Z","shell.execute_reply":"2025-10-02T20:19:33.972713Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# If failure, train on core corpus...","metadata":{}},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, get_peft_model\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# =====================================================================================\n# Configuration for Stage 1\n# =====================================================================================\nbase_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n# Path to your custom tokenizer\n# tokenizer_path = \"/kaggle/input/tokenizer-data/new_tk/new_tk\" \n\n\n\n\ntokenizer_path = base_model_id \n\n\n\n\n\n\n\n# Path to the folder containing all your medical textbook files. The '*' is a wildcard.\ncorpus_data_path = \"/kaggle/input/tokenizer-data/textbooks_en_jsonl/textbooks/en/*.jsonl\"\n# Where to save the pre-trained adapters from this stage\noutput_dir_stage1 = \"/kaggle/working/tinyllama-medical-pretrained\"\n\n# =====================================================================================\n# Step 1.1: Load Model and Tokenizer\n# =====================================================================================\nprint(\"--- STAGE 1: Loading Model and Tokenizer ---\")\n\n# Load your custom medical tokenizer\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n# A padding token is required for this training task\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Load the base model with 4-bit quantization for memory efficiency\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n)\n\n# CRITICAL: Resize model's vocabulary to match our custom tokenizer\nmodel.resize_token_embeddings(len(tokenizer))\nprint(f\"Model embedding resized to: {len(tokenizer)}\")\n\n# Configure LoRA for efficient pre-training\nlora_config = LoraConfig(\n    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:19:33.974710Z","iopub.execute_input":"2025-10-02T20:19:33.974993Z","iopub.status.idle":"2025-10-02T20:20:27.099708Z","shell.execute_reply.started":"2025-10-02T20:19:33.974977Z","shell.execute_reply":"2025-10-02T20:20:27.098988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================================================\n# Step 1.2: Load and Prepare the Corpus\n# =====================================================================================\nprint(\"\\n--- STAGE 1: Loading and Preparing the Corpus ---\")\n\n# Load all .jsonl files from the specified directory\nraw_dataset = load_dataset(\"json\", data_files=corpus_data_path, split=\"train\")\n\n# Tokenize the dataset (this will take a few minutes)\ntokenized_dataset = raw_dataset.map(\n    lambda examples: tokenizer(examples[\"text\"]),\n    batched=True, num_proc=4, remove_columns=[\"text\", \"source\"]\n)\n\n# Group texts into blocks for efficient language model training\nblock_size = 1024\ndef group_texts(examples):\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    total_length = (total_length // block_size) * block_size\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\nlm_dataset = tokenized_dataset.map(group_texts, batched=True, num_proc=4)\nprint(f\"Processed dataset with {len(lm_dataset)} blocks of size {block_size}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:20:27.103697Z","iopub.execute_input":"2025-10-02T20:20:27.103927Z","iopub.status.idle":"2025-10-02T20:21:15.551941Z","shell.execute_reply.started":"2025-10-02T20:20:27.103909Z","shell.execute_reply":"2025-10-02T20:21:15.551186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# =====================================================================================\n# Step 1.3: Run the Pre-training\n# =====================================================================================\nprint(\"\\n--- STAGE 1: Starting Continued Pre-training ---\")\n\n# Data collator for language modeling (predicting the next word)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir_stage1,\n    num_train_epochs=1, # One full pass over the large corpus is usually sufficient\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5, # A lower learning rate is crucial for stable pre-training\n    # save_steps=500,\n    save_strategy=\"epoch\",\n    logging_steps=100,\n    fp16=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_dataset,\n    data_collator=data_collator,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:21:30.069120Z","iopub.execute_input":"2025-10-02T20:21:30.069969Z","iopub.status.idle":"2025-10-02T20:21:30.454903Z","shell.execute_reply.started":"2025-10-02T20:21:30.069923Z","shell.execute_reply":"2025-10-02T20:21:30.454281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\nstart_time = time.time()\n\n# Start the training\ntrainer.train()\n\n\nend_time = time.time()\nprint(f\"Execution time: {end_time - start_time} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:21:36.799246Z","iopub.execute_input":"2025-10-02T20:21:36.799533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Save the resulting \"knowledge\" adapters\ntrainer.save_model(output_dir_stage1)\nprint(f\"\\n‚úÖ STAGE 1 COMPLETE! Medical knowledge adapters saved to {output_dir_stage1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T20:15:29.783813Z","iopub.execute_input":"2025-10-02T20:15:29.784551Z","iopub.status.idle":"2025-10-02T20:15:31.272527Z","shell.execute_reply.started":"2025-10-02T20:15:29.784523Z","shell.execute_reply":"2025-10-02T20:15:31.271729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, pipeline, LlamaTokenizerFast\nfrom peft import PeftModel\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"--- INTERMEDIATE TEST: Loading Stage 1 Model ---\")\n\n# =====================================================================================\n# Configuration for the Test\n# =====================================================================================\nbase_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n# Path to your custom tokenizer\n# tokenizer_path = \"/kaggle/input/tokenizer-data/new_tk/new_tk\"\n\n\n\ntokenizer_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n\n\n# Path to the adapters you just created in Stage 1\nstage1_adapters_path = \"/kaggle/working/tinyllama-medical-pretrained\"\n\n# =====================================================================================\n# Load and Merge the Stage 1 Model for Inference\n# =====================================================================================\n# Load the tokenizer\ntokenizer = LlamaTokenizerFast.from_pretrained(tokenizer_path)\n\n# Load the base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    load_in_8bit=True,\n    device_map=\"auto\",\n)\n# Resize embeddings\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Load the \"Knowledge Adapters\" from Stage 1\nmodel = PeftModel.from_pretrained(model, stage1_adapters_path)\n\n# Merge the adapters into the base model to make it a standalone model\nmodel = model.merge_and_unload()\nprint(\"‚úÖ Stage 1 model loaded and merged successfully.\")\n\n# =====================================================================================\n# Create Pipeline and Run Test\n# =====================================================================================\n# Create the text generation pipeline\nstage1_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100)\n\n# Define some test questions\ntest_questions = [\n    \"The common symptoms of diabetic ketoacidosis include\",\n    \"The pharmacokinetics of methotrexate are characterized by\"\n]\n\nprint(\"\\n--- Testing Stage 1 Model (Medical Knowledge) ---\")\nfor question_start in test_questions:\n    print(\"-\" * 50)\n    print(f\"‚ùì PROMPT: {question_start}...\")\n    \n    # This model was trained to complete text, not answer questions.\n    # We are testing if its completions are medically coherent.\n    result = stage1_pipe(question_start)\n    print(\"\\nüìö MODEL COMPLETION:\")\n    print(result[0]['generated_text'])\n    print(\"-\" * 50)\n\nprint(\"\\nIntermediate test complete. You can now proceed to Stage 2.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\n# IMPORTANT: Import the specific LlamaTokenizerFast class\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, LlamaTokenizerFast\nfrom peft import LoraConfig, get_peft_model, PeftModel\nfrom trl import SFTTrainer\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# =====================================================================================\n# Configuration for Stage 2\n# =====================================================================================\nbase_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n# üö® DOUBLE-CHECK THIS PATH! üö®\n# If you are in a NEW session, this MUST point to your Kaggle INPUT dataset, like:\n# stage1_adapters_path = \"/kaggle/input/tinyllama-medical-knowledge-adapters/tinyllama-medical-pretrained\"\nstage1_adapters_path = \"/kaggle/working/tinyllama-medical-pretrained\" # Only correct if in the SAME session as Stage 1\n\n# tokenizer_path = \"/kaggle/input/tokenizer-data/new_tk/new_tk\"\n\n\ntokenizer_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n\n\nqa_dataset_path = \"/kaggle/input/tokenizer-data/sleep_stress_dataset.json\"\noutput_dir_stage2 = \"/kaggle/working/tinyllama-medical-assistant-final\"\n\n# =====================================================================================\n# Step 2.1: Load, MERGE, and Prepare the Model\n# =====================================================================================\nprint(\"--- STAGE 2: Loading, MERGING, and Preparing ---\")\n\n# FIX #1: Use the specific LlamaTokenizerFast class\ntokenizer = LlamaTokenizerFast.from_pretrained(tokenizer_path)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Load the base model in 8-bit for the merging process\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    load_in_8bit=True, # Use 8-bit for merging\n    device_map=\"auto\",\n)\n\n# Resize embeddings\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Load the \"Knowledge Adapters\" from Stage 1\nmodel = PeftModel.from_pretrained(model, stage1_adapters_path)\nprint(\"Successfully loaded medical knowledge adapters.\")\n\n# FIX #2: MERGE the adapters into the base model to prevent RuntimeError\nmodel = model.merge_and_unload()\nprint(\"Adapters successfully merged into the base model.\")\n\n# Now, we apply a NEW LoRA config for the final fine-tuning stage\nlora_config = LoraConfig(\n    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# =====================================================================================\n# Step 2.2: Load Dataset and Configure Trainer\n# =====================================================================================\nprint(\"\\n--- STAGE 2: Configuring Trainer and Loading Q&A Dataset ---\")\ndataset = load_dataset(\"json\", data_files=qa_dataset_path, split=\"train\")\n\ndef format_prompt(sample):\n    return f\"\"\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}\"\"\"\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir_stage2,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    learning_rate=2e-5,\n    num_train_epochs=5,\n    logging_steps=5,\n    save_strategy=\"epoch\",\n    fp16=True,\n)\n\n# Create the SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=lora_config,\n    formatting_func=format_prompt,\n    args=training_args,\n    # max_seq_length=1024, # Re-added for best practice and memory management\n)\n\n# =====================================================================================\n# Step 2.3: Run the Final Training\n# =====================================================================================\nprint(\"\\n--- STAGE 2: Starting Final Instruction Fine-tuning ---\")\ntrainer.train()\ntrainer.save_model(output_dir_stage2)\nprint(f\"\\n‚úÖ STAGE 2 COMPLETE! Final medical assistant saved to {output_dir_stage2}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, pipeline, LlamaTokenizerFast\nfrom peft import PeftModel\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# =====================================================================================\n# Configuration: üö® DOUBLE-CHECK THESE PATHS! üö®\n# =====================================================================================\nbase_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n# Path to the tokenizer you trained\n# tokenizer_path = \"/kaggle/input/tokenizer-data/new_tk/new_tk\"\n\n\n\ntokenizer_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n\n\n\n# Path to the adapters from your \"Continued Pre-training\" run (Stage 1)\n# This might be in /kaggle/working/ or /kaggle/input/ depending on your session\nstage1_adapters_path = \"/kaggle/working/tinyllama-medical-pretrained\"\n\n# Path to the final adapters from your \"Instruction Fine-tuning\" run (Stage 2)\n# This might be in /kaggle/working/ or /kaggle/input/\nstage2_adapters_path = \"/kaggle/working/tinyllama-medical-assistant-final\"\n\n\n# =====================================================================================\n# Function to load and merge a PEFT model for inference\n# =====================================================================================\ndef load_and_merge_model(model_id, tokenizer_path, adapter_path):\n    print(f\"Loading and merging model from: {adapter_path}\")\n    tokenizer = LlamaTokenizerFast.from_pretrained(tokenizer_path)\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        load_in_8bit=True,\n        device_map=\"auto\",\n    )\n    model.resize_token_embeddings(len(tokenizer))\n    \n    model = PeftModel.from_pretrained(model, adapter_path)\n    model = model.merge_and_unload()\n    \n    print(\"‚úÖ Model loaded and merged successfully.\")\n    return model, tokenizer\n\n# =====================================================================================\n# Load All Three Models\n# =====================================================================================\n\n# --- 1. Base Model ---\nprint(\"Loading the original base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch.float16)\nbase_tokenizer = AutoTokenizer.from_pretrained(base_model_id)\nprint(\"‚úÖ Base model loaded.\")\n\n# --- 2. Stage 1 Model (Medical Knowledge) ---\nstage1_model, stage1_tokenizer = load_and_merge_model(base_model_id, tokenizer_path, stage1_adapters_path)\n\n# --- 3. Stage 2 Model (Medical Assistant Skill) ---\nstage2_model, stage2_tokenizer = load_and_merge_model(base_model_id, tokenizer_path, stage2_adapters_path)\n\n\n# =====================================================================================\n# Create Text Generation Pipelines\n# =====================================================================================\nprint(\"\\nCreating generation pipelines...\")\nbase_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=base_tokenizer, max_new_tokens=150)\nstage1_pipe = pipeline(\"text-generation\", model=stage1_model, tokenizer=stage1_tokenizer, max_new_tokens=150)\nstage2_pipe = pipeline(\"text-generation\", model=stage2_model, tokenizer=stage2_tokenizer, max_new_tokens=150)\nprint(\"‚úÖ Pipelines created.\")\n\n# =====================================================================================\n# Define Questions and Generate Responses\n# =====================================================================================\nquestions = [\n    \"What are the common symptoms of diabetic ketoacidosis?\",\n    \"I'm feeling very stressed and can't sleep. What steps can I take?\",\n    \"What is the capital of France?\" # A non-medical question to test for catastrophic forgetting\n]\n\nfor question in questions:\n    print(\"\\n\" + \"=\"*80)\n    print(f\"‚ùì QUESTION: {question}\")\n    print(\"=\"*80)\n\n    # --- 1. Base Model Response ---\n    prompt_base = f\"<|system|>\\nYou are a helpful assistant.</s>\\n<|user|>\\n{question}</s>\\n<|assistant|>\\n\"\n    result_base = base_pipe(prompt_base)\n    print(\"üì¢ BASE MODEL RESPONSE:\")\n    print(result_base[0]['generated_text'].split(\"<|assistant|>\")[1].strip())\n\n    # --- 2. Stage 1 Model Response ---\n    # This model was not trained to follow instructions, it was trained to complete text.\n    # It will \"talk like a textbook\" and might not answer the question directly.\n    prompt_stage1 = f\"A patient asks: {question}. A medical textbook would state:\"\n    result_stage1 = stage1_pipe(prompt_stage1)\n    print(\"\\n\\nüìö STAGE 1 (MEDICAL KNOWLEDGE) RESPONSE:\")\n    print(result_stage1[0]['generated_text'])\n\n    # --- 3. Stage 2 Model Response ---\n    # This model was trained to follow instructions in the Alpaca format.\n    prompt_stage2 = f\"### Instruction:\\n{question}\\n\\n### Response:\"\n    result_stage2 = stage2_pipe(prompt_stage2)\n    print(\"\\n\\nü©∫ STAGE 2 (FINAL ASSISTANT) RESPONSE:\")\n    print(result_stage2[0]['generated_text'].split(\"### Response:\")[1].strip())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}