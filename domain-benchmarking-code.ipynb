{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-03T13:45:53.252789Z",
     "iopub.status.busy": "2025-10-03T13:45:53.252280Z",
     "iopub.status.idle": "2025-10-03T13:45:53.509142Z",
     "shell.execute_reply": "2025-10-03T13:45:53.508543Z",
     "shell.execute_reply.started": "2025-10-03T13:45:53.252766Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/domain/sleep_stress_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:45:55.833862Z",
     "iopub.status.busy": "2025-10-03T13:45:55.833471Z",
     "iopub.status.idle": "2025-10-03T13:46:02.991512Z",
     "shell.execute_reply": "2025-10-03T13:46:02.990812Z",
     "shell.execute_reply.started": "2025-10-03T13:45:55.833839Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge-score) (2024.2.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ed6ceec22619d2902424161f9f3189b970c590268328362f8b03016f13bad575\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:46:06.171622Z",
     "iopub.status.busy": "2025-10-03T13:46:06.170997Z",
     "iopub.status.idle": "2025-10-03T13:46:42.445777Z",
     "shell.execute_reply": "2025-10-03T13:46:42.445015Z",
     "shell.execute_reply.started": "2025-10-03T13:46:06.171590Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 13:46:24.868287: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759499185.231915      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759499185.334612      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModel,\n",
    "    pipeline\n",
    ")\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(\"your_hf_token\") \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:46:42.447816Z",
     "iopub.status.busy": "2025-10-03T13:46:42.447256Z",
     "iopub.status.idle": "2025-10-03T13:46:42.452242Z",
     "shell.execute_reply": "2025-10-03T13:46:42.451410Z",
     "shell.execute_reply.started": "2025-10-03T13:46:42.447786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Disable TorchDynamo (fixes RecompileLimitExceeded)\n",
    "if hasattr(torch, \"_dynamo\"):\n",
    "    torch._dynamo.config.cache_size_limit = 64  # increase limit (optional)\n",
    "    torch._dynamo.config.suppress_errors = True\n",
    "    torch._dynamo.disable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:46:42.454393Z",
     "iopub.status.busy": "2025-10-03T13:46:42.454177Z",
     "iopub.status.idle": "2025-10-03T13:46:42.847505Z",
     "shell.execute_reply": "2025-10-03T13:46:42.846858Z",
     "shell.execute_reply.started": "2025-10-03T13:46:42.454377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:46:54.576294Z",
     "iopub.status.busy": "2025-10-03T13:46:54.575689Z",
     "iopub.status.idle": "2025-10-03T13:47:03.031269Z",
     "shell.execute_reply": "2025-10-03T13:47:03.030657Z",
     "shell.execute_reply.started": "2025-10-03T13:46:54.576268Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507e83a90008428699e0d46d5663bb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a1df1e44944f65a1f118f6cf21b085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c695c62859ce424aa112d97082a3b927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6223ab9acc14af38aae485a69f7784a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734d5e19dbd14e478cf7e923e064f8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7c05af53f5496b8f363649b2c5d6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_NAME)\n",
    "embed_model = AutoModel.from_pretrained(EMBED_MODEL_NAME).to(DEVICE)\n",
    "embed_model.eval()\n",
    "\n",
    "def embed(texts):\n",
    "    inputs = embed_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = embed_model(**inputs, return_dict=True)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings\n",
    "    \n",
    "def cosine_similarity(text1, text2):\n",
    "    emb1 = embed([text1])\n",
    "    emb2 = embed([text2])\n",
    "    return F.cosine_similarity(emb1, emb2).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:47:11.025671Z",
     "iopub.status.busy": "2025-10-03T13:47:11.025118Z",
     "iopub.status.idle": "2025-10-03T13:47:11.029053Z",
     "shell.execute_reply": "2025-10-03T13:47:11.028490Z",
     "shell.execute_reply.started": "2025-10-03T13:47:11.025646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "JUDGE_MODELS = {\n",
    "    \"Judge A: MedGemma - 4B\": \"google/medgemma-4b-it\", \n",
    "    \"Judge B: Meditron-7B\": \"epfl-llm/meditron-7b\",\n",
    "}\n",
    "MODELS = [\n",
    "    \"microsoft/phi-3-mini-4k-instruct\",     \n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",   \n",
    "    \"google/gemma-2b\",                       \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:47:14.575821Z",
     "iopub.status.busy": "2025-10-03T13:47:14.575318Z",
     "iopub.status.idle": "2025-10-03T13:47:14.579916Z",
     "shell.execute_reply": "2025-10-03T13:47:14.579120Z",
     "shell.execute_reply.started": "2025-10-03T13:47:14.575798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_judge(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    judge_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    return judge_pipeline, model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:47:17.799046Z",
     "iopub.status.busy": "2025-10-03T13:47:17.798457Z",
     "iopub.status.idle": "2025-10-03T13:47:17.803364Z",
     "shell.execute_reply": "2025-10-03T13:47:17.802790Z",
     "shell.execute_reply.started": "2025-10-03T13:47:17.799023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def judge_consistency(judge_pipeline, question, reference, response):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert fact-checking evaluator.\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Reference Answer (ground truth):\n",
    "    {reference}\n",
    "\n",
    "    Model Answer:\n",
    "    {response}\n",
    "\n",
    "    Task:\n",
    "    Evaluate if the Model Answer is factually consistent with the Reference Answer.\n",
    "    Respond with ONLY one of the following labels:\n",
    "    - \"consistent\"\n",
    "    - \"hallucinated\"\n",
    "    \"\"\"\n",
    "    result = judge_pipeline(prompt, max_new_tokens=50, do_sample=False, temperature=0.0)\n",
    "    verdict = result[0][\"generated_text\"].lower()\n",
    "\n",
    "    if \"consistent\" in verdict:\n",
    "        return \"consistent\"\n",
    "    elif \"hallucinated\" in verdict:\n",
    "        return \"hallucinated\"\n",
    "    else:\n",
    "        return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:47:25.770044Z",
     "iopub.status.busy": "2025-10-03T13:47:25.769527Z",
     "iopub.status.idle": "2025-10-03T13:47:25.778891Z",
     "shell.execute_reply": "2025-10-03T13:47:25.777942Z",
     "shell.execute_reply.started": "2025-10-03T13:47:25.770019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def benchmark_model_long_with_judge(model_name, data, judge_pipeline, max_samples=None):\n",
    "    print(f\"\\n=== Benchmarking {model_name} with LLM Judge ===\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    gen_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    results = []\n",
    "\n",
    "    for i, (qid, entry) in enumerate(data.items()):\n",
    "        if max_samples is not None and i >= max_samples:\n",
    "            break\n",
    "\n",
    "        question = entry[\"question\"].strip()\n",
    "        ref_answer = entry[\"reference\"].strip()\n",
    "        if not ref_answer:\n",
    "            continue\n",
    "\n",
    "        prompt = f\"Question: {question}\\nAnswer in detail:\"\n",
    "\n",
    "        response = gen_pipeline(prompt, max_new_tokens=200, do_sample=False)\n",
    "        model_answer = response[0][\"generated_text\"].replace(prompt, \"\").strip()\n",
    "\n",
    "        sim = cosine_similarity(ref_answer, model_answer)\n",
    "        rouge_score = rouge.score(ref_answer, model_answer)[\"rougeL\"].fmeasure\n",
    "        verdict = judge_consistency(judge_pipeline, question, ref_answer, model_answer)\n",
    "\n",
    "        results.append({\n",
    "            \"id\": qid,\n",
    "            \"similarity\": sim,\n",
    "            \"rougeL\": rouge_score,\n",
    "            \"verdict\": verdict,\n",
    "        })\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i+1} datapoints...\")\n",
    "\n",
    "    avg_sim = sum(r[\"similarity\"] for r in results) / len(results)\n",
    "    avg_rouge = sum(r[\"rougeL\"] for r in results) / len(results)\n",
    "    verdict_counts = {\n",
    "        v: sum(r[\"verdict\"] == v for r in results)\n",
    "        for v in [\"consistent\", \"hallucinated\", \"unknown\"]\n",
    "    }\n",
    "\n",
    "    print(f\"Samples Evaluated: {len(results)}\")\n",
    "    print(f\"Avg Semantic Similarity: {avg_sim:.3f}\")\n",
    "    print(f\"Avg ROUGE-L: {avg_rouge:.3f}\")\n",
    "    print(f\"Judge Verdicts: {verdict_counts}\")\n",
    "\n",
    "    del model, tokenizer, gen_pipeline\n",
    "    cleanup()\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"avg_similarity\": round(avg_sim, 3),\n",
    "        \"avg_rouge\": round(avg_rouge, 3),\n",
    "        \"samples\": len(results),\n",
    "        \"verdicts\": verdict_counts\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:47:32.513462Z",
     "iopub.status.busy": "2025-10-03T13:47:32.513190Z",
     "iopub.status.idle": "2025-10-03T13:47:32.532767Z",
     "shell.execute_reply": "2025-10-03T13:47:32.532016Z",
     "shell.execute_reply.started": "2025-10-03T13:47:32.513444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/domain/sleep_stress_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "data = {\n",
    "    str(i): {\n",
    "        \"question\": entry[\"instruction\"] + (\"\\n\" + entry[\"input\"] if entry[\"input\"] else \"\"),\n",
    "        \"reference\": entry[\"output\"]\n",
    "    }\n",
    "    for i, entry in enumerate(raw_data)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T11:24:43.832119Z",
     "iopub.status.busy": "2025-10-03T11:24:43.831883Z",
     "iopub.status.idle": "2025-10-03T13:08:22.788027Z",
     "shell.execute_reply": "2025-10-03T13:08:22.783719Z",
     "shell.execute_reply.started": "2025-10-03T11:24:43.832091Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "########## Using Judge A: MedGemma - 4B (google/medgemma-4b-it) ##########\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b137208ec4cb4005b846551328234319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14781899ae20459b87bd7e51f87835be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f8daad630a47ed83e52a81cc19f272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29c932e47434cbb863335f3f5a225d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab06e24c14f4a409b4a7057d8f2862f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7837e7c3c94b4d5686b3b1521759bd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b054097eae8f4ff7968145a2692d0af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf1446387674646a2bf0499b236ad2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77916be82c05489f990929744f606c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8adb9ccfe0f48e589cf9bfdf595031f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805ccb57186a470ebb1093b4ffa0b2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c883c6bf9f4d4fb4a6f7509a6e6533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05441462b486457e816e27b8d7b19ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Benchmarking microsoft/phi-3-mini-4k-instruct with LLM Judge ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37786511f084446fb824b47f5cf6ab40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a65a125ac543c085fa4f9b57629163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56939217d0a4bf7ac471065d568e9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd019d9d42e484691fcd9985f72688b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce865a9b8f046079b5bec72ae5ef866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d2e84f77094160b7a66ce7705b50de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5064ee428143feae764d4bdd600685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cceda6047fa14b57888d3752b1eb0a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382a602488494155821cf93aaa84f34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3005299bb784d66bfb60a615bd283dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fca4625283b4f07a74238a9e80246db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767aadcb9b6440c6ab10e2c05cea696b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1003 11:28:33.153000 36 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W1003 11:28:33.176000 36 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "skipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n",
      "skipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n",
      "skipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n",
      "skipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 datapoints...\n",
      "Processed 20 datapoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n",
      "skipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30 datapoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40 datapoints...\n",
      "Processed 50 datapoints...\n",
      "Processed 60 datapoints...\n",
      "Processed 70 datapoints...\n",
      "Processed 80 datapoints...\n",
      "Processed 90 datapoints...\n",
      "Processed 100 datapoints...\n",
      "Processed 110 datapoints...\n",
      "Processed 120 datapoints...\n",
      "Processed 130 datapoints...\n",
      "Processed 140 datapoints...\n",
      "Processed 150 datapoints...\n",
      "Processed 160 datapoints...\n",
      "Processed 170 datapoints...\n",
      "Processed 180 datapoints...\n",
      "Processed 190 datapoints...\n",
      "Processed 200 datapoints...\n",
      "Samples Evaluated: 200\n",
      "Avg Semantic Similarity: 0.721\n",
      "Avg ROUGE-L: 0.113\n",
      "Judge Verdicts: {'consistent': 200, 'hallucinated': 0, 'unknown': 0}\n",
      "\n",
      "=== Benchmarking TinyLlama/TinyLlama-1.1B-Chat-v1.0 with LLM Judge ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13470377126f41cdb74c83f82408a527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0b5e125f6d424884dcbb544e37a14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9ee64da0104b6aba6cf27bf40c0aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85716b82d40349f5af8f5e7e0efc96be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a41e7ce164240b0a3f16dfcbc81f0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5add58be384982a1f30517aef32b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7414173484f418ba698e7bd5ab1f5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 datapoints...\n",
      "Processed 20 datapoints...\n",
      "Processed 30 datapoints...\n",
      "Processed 40 datapoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 datapoints...\n",
      "Processed 60 datapoints...\n",
      "Processed 70 datapoints...\n",
      "Processed 80 datapoints...\n",
      "Processed 90 datapoints...\n",
      "Processed 100 datapoints...\n",
      "Processed 110 datapoints...\n",
      "Processed 120 datapoints...\n",
      "Processed 130 datapoints...\n",
      "Processed 140 datapoints...\n",
      "Processed 150 datapoints...\n",
      "Processed 160 datapoints...\n",
      "Processed 170 datapoints...\n",
      "Processed 180 datapoints...\n",
      "Processed 190 datapoints...\n",
      "Processed 200 datapoints...\n",
      "Samples Evaluated: 200\n",
      "Avg Semantic Similarity: 0.704\n",
      "Avg ROUGE-L: 0.131\n",
      "Judge Verdicts: {'consistent': 200, 'hallucinated': 0, 'unknown': 0}\n",
      "\n",
      "=== Benchmarking google/gemma-2b with LLM Judge ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f9e7e0477e4ca391ca861dbc07188c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e5b515eb004f408df20bfcae16f558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456d791c5f674518a2d86bd0a8d55382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8f12dfc62c41fda6ccf9d540c16692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d60567daf6f4232aff9f0d4ca92e0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48319d8510943d0a23f3f0223c1f1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc3c95babf647e2a95d068c98fa5338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba7bd88f7b24c7d8b7cfec8f70bf2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a664f0fedbb44569f30777e527885c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf448aaf2e144a080d7e619d314782a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1bd4f9e43749daad4ca662527a4827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 datapoints...\n",
      "Processed 20 datapoints...\n",
      "Processed 30 datapoints...\n",
      "Processed 40 datapoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 datapoints...\n",
      "Processed 60 datapoints...\n",
      "Processed 70 datapoints...\n",
      "Processed 80 datapoints...\n",
      "Processed 90 datapoints...\n",
      "Processed 100 datapoints...\n",
      "Processed 110 datapoints...\n",
      "Processed 120 datapoints...\n",
      "Processed 130 datapoints...\n",
      "Processed 140 datapoints...\n",
      "Processed 150 datapoints...\n",
      "Processed 160 datapoints...\n",
      "Processed 170 datapoints...\n",
      "Processed 180 datapoints...\n",
      "Processed 190 datapoints...\n",
      "Processed 200 datapoints...\n",
      "Samples Evaluated: 200\n",
      "Avg Semantic Similarity: 0.642\n",
      "Avg ROUGE-L: 0.081\n",
      "Judge Verdicts: {'consistent': 200, 'hallucinated': 0, 'unknown': 0}\n"
     ]
    }
   ],
   "source": [
    "leaderboards = {}\n",
    "\n",
    "judge_label = \"Judge A: MedGemma - 4B\"\n",
    "judge_model_id = \"google/medgemma-4b-it\"\n",
    "print(f\"\\n\\n########## Using {judge_label} ({judge_model_id}) ##########\\n\")\n",
    "judge_pipeline, judge_model, judge_tokenizer = load_judge(judge_model_id)\n",
    "\n",
    "leaderboard = []\n",
    "for model_id in MODELS:\n",
    "    summary = benchmark_model_long_with_judge(\n",
    "        model_id, data, judge_pipeline, max_samples=200\n",
    "    )\n",
    "    leaderboard.append(summary)\n",
    "\n",
    "leaderboards[judge_label] = leaderboard\n",
    "\n",
    "del judge_model, judge_tokenizer, judge_pipeline\n",
    "cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:43:11.566298Z",
     "iopub.status.busy": "2025-10-03T13:43:11.566026Z",
     "iopub.status.idle": "2025-10-03T13:43:14.341750Z",
     "shell.execute_reply": "2025-10-03T13:43:14.340923Z",
     "shell.execute_reply.started": "2025-10-03T13:43:11.566280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                                              Size  Used Avail Use% Mounted on\n",
      "overlay                                                                 7.9T  6.3T  1.7T  80% /\n",
      "tmpfs                                                                    64M     0   64M   0% /dev\n",
      "shm                                                                      14G   24K   14G   1% /dev/shm\n",
      "/dev/loop1                                                               20G   76K   20G   1% /kaggle/lib\n",
      "192.168.5.2:/data/kagglesdsdata/datasets/8391833/13243919/dd8191qiwlg1   73T   54T   20T  74% /kaggle/input/domain\n",
      "/dev/sda1                                                               122G   90G   33G  74% /opt/bin\n",
      "/dev/mapper/snap                                                        7.9T  6.3T  1.7T  80% /etc/hosts\n",
      "tmpfs                                                                    16G     0   16G   0% /proc/acpi\n",
      "tmpfs                                                                    16G     0   16G   0% /proc/scsi\n",
      "tmpfs                                                                    16G     0   16G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "# Clean Hugging Face cache and temp files\n",
    "!rm -rf /root/.cache/huggingface/hub/*\n",
    "!rm -rf /kaggle/temp/*\n",
    "\n",
    "# Check available disk\n",
    "!df -h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:38:18.610329Z",
     "iopub.status.busy": "2025-10-03T13:38:18.610040Z",
     "iopub.status.idle": "2025-10-03T13:38:18.615252Z",
     "shell.execute_reply": "2025-10-03T13:38:18.614597Z",
     "shell.execute_reply.started": "2025-10-03T13:38:18.610306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/kaggle/working/leaderboards_partial.json\", \"w\") as f:\n",
    "    json.dump(leaderboards, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:53:19.006861Z",
     "iopub.status.busy": "2025-10-03T13:53:19.006580Z",
     "iopub.status.idle": "2025-10-03T13:53:19.013263Z",
     "shell.execute_reply": "2025-10-03T13:53:19.012453Z",
     "shell.execute_reply.started": "2025-10-03T13:53:19.006840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/leaderboard/leaderboards_partial.json\", \"r\") as f:\n",
    "    leaderboards = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:53:21.910581Z",
     "iopub.status.busy": "2025-10-03T13:53:21.910314Z",
     "iopub.status.idle": "2025-10-03T13:53:21.914793Z",
     "shell.execute_reply": "2025-10-03T13:53:21.913965Z",
     "shell.execute_reply.started": "2025-10-03T13:53:21.910559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Judge A: MedGemma - 4B': [{'model': 'microsoft/phi-3-mini-4k-instruct', 'avg_similarity': 0.721, 'avg_rouge': 0.113, 'samples': 200, 'verdicts': {'consistent': 200, 'hallucinated': 0, 'unknown': 0}}, {'model': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'avg_similarity': 0.704, 'avg_rouge': 0.131, 'samples': 200, 'verdicts': {'consistent': 200, 'hallucinated': 0, 'unknown': 0}}, {'model': 'google/gemma-2b', 'avg_similarity': 0.642, 'avg_rouge': 0.081, 'samples': 200, 'verdicts': {'consistent': 200, 'hallucinated': 0, 'unknown': 0}}]}\n"
     ]
    }
   ],
   "source": [
    "print(leaderboards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T13:53:30.603494Z",
     "iopub.status.busy": "2025-10-03T13:53:30.603217Z",
     "iopub.status.idle": "2025-10-03T15:30:06.538203Z",
     "shell.execute_reply": "2025-10-03T15:30:06.534114Z",
     "shell.execute_reply.started": "2025-10-03T13:53:30.603473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "########## Using Judge B: Meditron-7B (epfl-llm/meditron-7b) ##########\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ea93bdb11549678738acc61b4e5016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be081d23e923473a84a843027102f5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9043bf51da364690a4e140023dd1ffe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e1465b55b7490b9f25501507b56e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/344 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6223f02335d485abaa38d09368b07b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/736 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865f9c0d48b04b30b0e9795632f82f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb149b7249940f2b3c1bf28c28d4b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84498be43a24600827de0221fc3710f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2619704d935349cfa97f34be52a50d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77399882bd1247edbcebefc17323a3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/1.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e206b17226491aaa1d2ef60c32b14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/1.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5b0520d1ce45b4b3e5ade0015c4741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7006a8208845769feb558899fbb422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e548140f9a4643a69f27b10ccde8fc26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4de36cb3b9740c5b97b3735a24c7d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/262M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f15614c9cc45bc9a672e039c4aab88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/1.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac3e55d066e44b8ba63cdcee7c4fe97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ac4d40aa714030bf81548188e7fd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Benchmarking microsoft/phi-3-mini-4k-instruct with LLM Judge ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff0d7a11e7545168b8e1e25ae0c7fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e0f5b467bd47e7b3daea75760a2f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa4d4ce7c8b4b998650b90dc4b0cf0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a03d249fd9141b98c7d87b9f5eabf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6cea8045544287b5d8e17dd1d0fde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e778eed5bc04a63b54149bd27412806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b4036b31d9418497d9908ba64fe18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f3e06e54894141a4b99df785834ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136e383c72f54e67bba0ede4befa80f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf3af63885c4b50b389680efd2eec4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895f7e77716447049672ea0646b48c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d09e178c5384b4b85d1d981371691fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 datapoints...\n",
      "Processed 20 datapoints...\n",
      "Processed 30 datapoints...\n",
      "Processed 40 datapoints...\n",
      "Processed 50 datapoints...\n",
      "Processed 60 datapoints...\n",
      "Processed 70 datapoints...\n",
      "Processed 80 datapoints...\n",
      "Processed 90 datapoints...\n",
      "Processed 100 datapoints...\n",
      "Processed 110 datapoints...\n",
      "Processed 120 datapoints...\n",
      "Processed 130 datapoints...\n",
      "Processed 140 datapoints...\n",
      "Processed 150 datapoints...\n",
      "Processed 160 datapoints...\n",
      "Processed 170 datapoints...\n",
      "Processed 180 datapoints...\n",
      "Processed 190 datapoints...\n",
      "Processed 200 datapoints...\n",
      "Samples Evaluated: 200\n",
      "Avg Semantic Similarity: 0.721\n",
      "Avg ROUGE-L: 0.113\n",
      "Judge Verdicts: {'consistent': 200, 'hallucinated': 0, 'unknown': 0}\n",
      "\n",
      "=== Benchmarking TinyLlama/TinyLlama-1.1B-Chat-v1.0 with LLM Judge ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c66bb03d3647df88e92f1da59a167c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce32284aea7f49c6adcf759e49e06b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaff50253118468ea0391d45b2546ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ea5eb32ac541dda7dd74861485110b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0406ca43a644f86b45af49a30756832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5429261bcf41c09e93d05c75092dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6204d48880744affbff94efbd05553b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 datapoints...\n",
      "Processed 20 datapoints...\n",
      "Processed 30 datapoints...\n",
      "Processed 40 datapoints...\n",
      "Processed 50 datapoints...\n",
      "Processed 60 datapoints...\n",
      "Processed 70 datapoints...\n",
      "Processed 80 datapoints...\n",
      "Processed 90 datapoints...\n",
      "Processed 100 datapoints...\n",
      "Processed 110 datapoints...\n",
      "Processed 120 datapoints...\n",
      "Processed 130 datapoints...\n",
      "Processed 140 datapoints...\n",
      "Processed 150 datapoints...\n",
      "Processed 160 datapoints...\n",
      "Processed 170 datapoints...\n",
      "Processed 180 datapoints...\n",
      "Processed 190 datapoints...\n",
      "Processed 200 datapoints...\n",
      "Samples Evaluated: 200\n",
      "Avg Semantic Similarity: 0.704\n",
      "Avg ROUGE-L: 0.131\n",
      "Judge Verdicts: {'consistent': 200, 'hallucinated': 0, 'unknown': 0}\n",
      "\n",
      "=== Benchmarking google/gemma-2b with LLM Judge ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821e711b5ef447d5b14cc9856b97d391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bda493086d48509cfb7e4a9e9d2021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dabebe30f284f1da7234f024653cc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec33c6c90871409dad121123b746f98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c54073359d24d639a88f3aa2840f322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0c29803d7f4a16947d13320f5496b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a1446ec92a429d88ed74738b860320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16633425ec0b422185961ec9e0f47611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337723c2e2594f3ea9d63ecd79f8ce62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79efbc6536274360bdf6ab642df49c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5b4fc692cf4827a111a883ac19ea71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 datapoints...\n",
      "Processed 20 datapoints...\n",
      "Processed 30 datapoints...\n",
      "Processed 40 datapoints...\n",
      "Processed 50 datapoints...\n",
      "Processed 60 datapoints...\n",
      "Processed 70 datapoints...\n",
      "Processed 80 datapoints...\n",
      "Processed 90 datapoints...\n",
      "Processed 100 datapoints...\n",
      "Processed 110 datapoints...\n",
      "Processed 120 datapoints...\n",
      "Processed 130 datapoints...\n",
      "Processed 140 datapoints...\n",
      "Processed 150 datapoints...\n",
      "Processed 160 datapoints...\n",
      "Processed 170 datapoints...\n",
      "Processed 180 datapoints...\n",
      "Processed 190 datapoints...\n",
      "Processed 200 datapoints...\n",
      "Samples Evaluated: 200\n",
      "Avg Semantic Similarity: 0.642\n",
      "Avg ROUGE-L: 0.081\n",
      "Judge Verdicts: {'consistent': 200, 'hallucinated': 0, 'unknown': 0}\n"
     ]
    }
   ],
   "source": [
    "judge_label = \"Judge B: Meditron-7B\"\n",
    "judge_model_id = \"epfl-llm/meditron-7b\"\n",
    "print(f\"\\n\\n########## Using {judge_label} ({judge_model_id}) ##########\\n\")\n",
    "judge_pipeline, judge_model, judge_tokenizer = load_judge(judge_model_id)\n",
    "\n",
    "leaderboard = []\n",
    "for model_id in MODELS:\n",
    "    summary = benchmark_model_long_with_judge(\n",
    "        model_id, data, judge_pipeline, max_samples=200\n",
    "    )\n",
    "    leaderboard.append(summary)\n",
    "\n",
    "leaderboards[judge_label] = leaderboard\n",
    "\n",
    "del judge_model, judge_tokenizer, judge_pipeline\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T15:30:06.559388Z",
     "iopub.status.busy": "2025-10-03T15:30:06.559186Z",
     "iopub.status.idle": "2025-10-03T15:30:06.566348Z",
     "shell.execute_reply": "2025-10-03T15:30:06.565593Z",
     "shell.execute_reply.started": "2025-10-03T15:30:06.559372Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Leaderboard (Judge A: MedGemma - 4B) ===\n",
      "Model                                    |    Sim |  ROUGE-L |  Samples | Consistent |   Halluc |  Unknown\n",
      "----------------------------------------------------------------------------------------------------\n",
      "microsoft/phi-3-mini-4k-instruct         |  0.721 |    0.113 |      200 |        200 |        0 |        0\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0       |  0.704 |    0.131 |      200 |        200 |        0 |        0\n",
      "google/gemma-2b                          |  0.642 |    0.081 |      200 |        200 |        0 |        0\n",
      "\n",
      "=== Leaderboard (Judge B: Meditron-7B) ===\n",
      "Model                                    |    Sim |  ROUGE-L |  Samples | Consistent |   Halluc |  Unknown\n",
      "----------------------------------------------------------------------------------------------------\n",
      "microsoft/phi-3-mini-4k-instruct         |  0.721 |    0.113 |      200 |        200 |        0 |        0\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0       |  0.704 |    0.131 |      200 |        200 |        0 |        0\n",
      "google/gemma-2b                          |  0.642 |    0.081 |      200 |        200 |        0 |        0\n"
     ]
    }
   ],
   "source": [
    "for judge_label, leaderboard in leaderboards.items():\n",
    "    print(f\"\\n=== Leaderboard ({judge_label}) ===\")\n",
    "    print(f\"{'Model':40s} | {'Sim':>6s} | {'ROUGE-L':>8s} | {'Samples':>8s} | {'Consistent':>10s} | {'Halluc':>8s} | {'Unknown':>8s}\")\n",
    "    print(\"-\" * 100)\n",
    "    for row in leaderboard:\n",
    "        print(f\"{row['model']:40s} | {row['avg_similarity']:6.3f} | {row['avg_rouge']:8.3f} | {row['samples']:8d} | \"\n",
    "              f\"{row['verdicts']['consistent']:10d} | {row['verdicts']['hallucinated']:8d} | {row['verdicts']['unknown']:8d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T15:44:48.077145Z",
     "iopub.status.busy": "2025-10-03T15:44:48.076386Z",
     "iopub.status.idle": "2025-10-03T15:44:48.081286Z",
     "shell.execute_reply": "2025-10-03T15:44:48.080521Z",
     "shell.execute_reply.started": "2025-10-03T15:44:48.077118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/kaggle/working/leaderboards_full.json\", \"w\") as f:\n",
    "    json.dump(leaderboards, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8391833,
     "sourceId": 13243919,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8396990,
     "sourceId": 13251606,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
